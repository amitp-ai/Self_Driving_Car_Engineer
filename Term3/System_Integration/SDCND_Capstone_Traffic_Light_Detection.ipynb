{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Nanodegree Final Capstone Project\n",
    "\n",
    "\n",
    "## Traffic Light Detector: Using Faster-R-CNN\n",
    "\n",
    "This notebook is used to train a faster R-CNN network for traffic light detection.\n",
    ">** Note: The trained model will then be used in a real car by communicating with other modules such as waypoints updater etc **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Dataset Exploration\n",
    "\n",
    "The dataset used is:\n",
    "    1. Bosch Small Traffic Light Dataset\n",
    "    2. Udacity Simulator Dataset\n",
    "    \n",
    "Visualize the Dataset. This is implemented by plotting traffic light images, plotting the count of each sign, etc.\n",
    "\n",
    "\n",
    "- The training dataset images are in rgb format:\n",
    "    - Will determine the width, height, and channel count of the image\n",
    "- The training labels are in a *.yaml file containing the following information:\n",
    "    - Traffic light color\n",
    "    - Bounding box coordinates for each traffic light\n",
    "\n",
    "Note: Because of bounding box ground truth coordinates, the training input image is not resized. It is kept as is so the bounding box coordinates match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the needed libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2 #for generating additional images (by jittering existing images)\n",
    "import random #to randomize the training dataset\n",
    "#\n",
    "import math\n",
    "import time\n",
    "#\n",
    "import sys #for saving/opening files\n",
    "import os #for saving/opening files e.g. saving tensorflow variables\n",
    "import yaml #to opem yaml files\n",
    "#\n",
    "%matplotlib inline\n",
    "#print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sample script to receive traffic light labels and images\n",
    "of the Bosch Small Traffic Lights Dataset.\n",
    "Most of this is borrowed from Bosch traffic light dataset github (https://github.com/bosch-ros-pkg)\n",
    "\"\"\"\n",
    "def get_all_labels(input_yaml, riib=False):\n",
    "    \"\"\" Gets all labels within label file\n",
    "    Note that RGB images are 1280x720 and RIIB images are 1280x736.\n",
    "    :param input_yaml: Path to yaml file\n",
    "    :param riib: If True, change path to labeled pictures\n",
    "    :return: images: Labels for traffic lights\n",
    "    \"\"\"\n",
    "    images = yaml.load(open(input_yaml, 'rb').read())\n",
    "\n",
    "    for i in range(len(images)):\n",
    "        images[i]['path'] = os.path.abspath(os.path.join(os.path.dirname(input_yaml), images[i]['path']))\n",
    "        if riib:\n",
    "            images[i]['path'] = images[i]['path'].replace('.png', '.pgm')\n",
    "            images[i]['path'] = images[i]['path'].replace('rgb/train', 'riib/train')\n",
    "            images[i]['path'] = images[i]['path'].replace('rgb/test', 'riib/test')\n",
    "            for box in images[i]['boxes']:\n",
    "                box['y_max'] = box['y_max'] + 8\n",
    "                box['y_min'] = box['y_min'] + 8\n",
    "    return images #images is a yaml list. Not actual image files.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This method displays the traffic light labels within\n",
    "the given images.\n",
    "If given an output folder, it draws them to file.\n",
    "\"\"\"\n",
    "def ir(some_value):\n",
    "    \"\"\"Int-round function for short array indexing \"\"\"\n",
    "    return int(round(some_value))\n",
    "\n",
    "\n",
    "def show_label_images(images_list, output_folder=None):\n",
    "    \"\"\"\n",
    "    Shows and draws pictures with labeled traffic lights.\n",
    "    Can save pictures.\n",
    "    :param input_yaml: Path to yaml file\n",
    "    :param output_folder: If None, do not save picture. Else enter path to folder\n",
    "    \"\"\"\n",
    "\n",
    "    if output_folder is not None:\n",
    "        if not os.path.exists(output_folder):\n",
    "            os.makedirs(output_folder)\n",
    "\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX #font for displaying the label\n",
    "    for i, image_dict in enumerate(images_list):\n",
    "        image = cv2.imread(image_dict['path'])\n",
    "        if image is None:\n",
    "            raise IOError('Could not open image path', image_dict['path'])\n",
    "\n",
    "        for box in image_dict['boxes']:\n",
    "            cv2.rectangle(image,\n",
    "                          (ir(box['x_min']), ir(box['y_min'])),\n",
    "                          (ir(box['x_max']), ir(box['y_max'])),\n",
    "                          (0, 255, 0))\n",
    "            cv2.putText(image,box['label'],\n",
    "                        (ir(box['x_min']),ir(box['y_min'])),\n",
    "                        font, 0.5,(0,0,255),1)\n",
    "    \n",
    "\n",
    "        #cv2.imshow('labeled_image', image)\n",
    "        #cv2.waitKey(10)\n",
    "        #cv2.destroyAllWindows()\n",
    "        if output_folder is not None:\n",
    "            cv2.imwrite(os.path.join(output_folder, str(i).zfill(10) + '_'\n",
    "                        + os.path.basename(image_dict['path'])), image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "training_file_yaml = './data/mini_train_data/train.yaml' #yaml file containing image path, bounding box, and traffic light solor\n",
    "\n",
    "#Generate training data list\n",
    "training_data_list = get_all_labels(training_file_yaml)\n",
    "random.shuffle(training_data_list) #randomize the training data list\n",
    "\n",
    "print(len(training_data_list))\n",
    "print(type(training_data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train only using 1000 randomly chosen images to speed up training time\n",
    "num_training_image = 20 #1000\n",
    "limit_training_data = False\n",
    "if limit_training_data:\n",
    "    training_data_list = training_data_list[0:num_training_image]\n",
    "\n",
    "#This will add bounding box to each image in a small sample of the training dataset and display & save in the output folder\n",
    "num_images_to_view = 10\n",
    "view_images_list = training_data_list[0:num_images_to_view]\n",
    "output_folder = 'labelled_training_data_with_bbox/'\n",
    "show_label_images(view_images_list, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "- use functor/generator to send training data to the netwrok\n",
    "- preprocess the input image (mean shift and scaling)\n",
    "- don't rescale the input image. but need to figure out how to handle different input image sizes during testing. I.e. know the size of image sent by the camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f #to reset memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To start off let's do a basic data summary.\n",
    "\n",
    "# TODO: number of training examples\n",
    "n_train = X_train.shape[0]\n",
    "\n",
    "# TODO: number of testing examples\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# TODO: what's the shape of an image?\n",
    "image_shape = X_test.shape[1:-1]\n",
    "\n",
    "# TODO: how many classes are in the dataset\n",
    "n_classes = np.max(y_train) + 1 #as zero is a class type\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_fake_data_generated = False #Initially it's not generated\n",
    "use_fake_data = True \n",
    "is_fake_data_concatenated = False #Initialy its not concatenated\n",
    "is_features_normal = False #Initially the input features are not normalized\n",
    "is_labels_encod = False #Initially there is no 'one-hot' encoding.\n",
    "is_training_data_shuffled = False #Initially its not suffled.\n",
    "is_test_data_shuffled = False #Initially its not suffled.\n",
    "is_validation_data = False #Initially we don't have any validation data\n",
    "\n",
    "standardize_image = False #to do standardized pre-processing (note: must choose either standardization or normalization but not both)\n",
    "normalize_image = True #to do normalized preprocessing (note: must choose either standardization or normalization but not both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data exploration visualization goes here.\n",
    "\n",
    "#This is a function to plot any number of images\n",
    "def plot_images(pltimages, true_labels, pred_labels=None):\n",
    "    \n",
    "    subplt_dim = math.ceil(math.sqrt(pltimages.shape[0]))\n",
    "    \n",
    "    # Create figure with subplt_dim x subplt_dim sub-plots.\n",
    "    fig, axes = plt.subplots(subplt_dim, subplt_dim)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "    \n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "        # Plot image if i is less than image length.\n",
    "        if i >= pltimages.shape[0]:\n",
    "            continue\n",
    "        ax.imshow(pltimages[i])\n",
    "        \n",
    "        # Show true and predicted classes.\n",
    "        if pred_labels is None:\n",
    "            xlabel = \"True: {0}\".format(true_labels[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(true_labels[i], pred_labels[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n images from the entire training set in sequence or in random\n",
    "n_img = 9\n",
    "n_img_array = range(0,n_img) #in sequence\n",
    "#n_img_array = np.random.randint(0, high=n_train, size=n_img) #in random\n",
    "plot_images(X_train[n_img_array], y_train[n_img_array])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Step 2: Design and Test a Model Architecture\n",
    "\n",
    "Design and implement a deep learning model that learns to recognize traffic signs. Train and test your model on the [German Traffic Sign Dataset](http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset).\n",
    "\n",
    "There are various aspects to consider when thinking about this problem:\n",
    "\n",
    "- Your model can be derived from a deep feedforward net or a deep convolutional network.\n",
    "- Play around preprocessing techniques (normalization, rgb to grayscale, etc)\n",
    "- Number of examples per label (some have more than others).\n",
    "- Generate fake data.\n",
    "\n",
    "Here is an example of a [published baseline model on this problem](http://yann.lecun.com/exdb/publis/pdf/sermanet-ijcnn-11.pdf). It's not required to be familiar with the approach used in the paper but, it's good practice to try to read papers like these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine how many images are of each class in the training set\n",
    "def num_of_each_classes(y_input_labels):\n",
    "    label_list = np.zeros([43])\n",
    "    for ii in y_input_labels:\n",
    "        label_list[ii] += 1\n",
    "    return label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This function will take in an image and randomly perturb it (rotation, translations, and shear) to generate fake data\n",
    "#The below function was taken from Vivek Yadav's Medium blogpost\n",
    "def transform_image(img,ang_range,shear_range,trans_range):\n",
    "    '''\n",
    "    This function transforms images to generate new images.\n",
    "    The function takes in following arguments,\n",
    "    1- Image\n",
    "    2- ang_range: Range of angles for rotation\n",
    "    3- shear_range: Range of values to apply affine transform to\n",
    "    4- trans_range: Range of values to apply translations over. \n",
    "    \n",
    "    A Random uniform distribution is used to generate different parameters for transformation\n",
    "    \n",
    "    '''\n",
    "    # Rotation\n",
    "\n",
    "    ang_rot = np.random.uniform(ang_range)-ang_range/2\n",
    "    rows,cols,ch = img.shape\n",
    "    #cv2.getRotationMatrix2D(center, angle, scale)\n",
    "    Rot_M = cv2.getRotationMatrix2D((cols/2,rows/2),ang_rot,1)\n",
    "\n",
    "    # Translation\n",
    "    tr_x = trans_range*np.random.uniform()-trans_range/2\n",
    "    tr_y = trans_range*np.random.uniform()-trans_range/2\n",
    "    Trans_M = np.float32([[1,0,tr_x],[0,1,tr_y]])\n",
    "\n",
    "    # Shear\n",
    "    pts1 = np.float32([[5,5],[20,5],[5,20]])\n",
    "\n",
    "    pt1 = 5+shear_range*np.random.uniform()-shear_range/2\n",
    "    pt2 = 20+shear_range*np.random.uniform()-shear_range/2\n",
    "\n",
    "    pts2 = np.float32([[pt1,5],[pt2,pt1],[5,pt2]])\n",
    "\n",
    "    shear_M = cv2.getAffineTransform(pts1,pts2)\n",
    "        \n",
    "    img = cv2.warpAffine(img,Rot_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,Trans_M,(cols,rows))\n",
    "    img = cv2.warpAffine(img,shear_M,(cols,rows))\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(is_fake_data_generated) and (use_fake_data):\n",
    "    \n",
    "    y_train_num_each_classes = num_of_each_classes(y_train) #array of the number of each class\n",
    "    num_fake_data_per_class = 2500 - y_train_num_each_classes\n",
    "    num_fake_data_array = np.ceil(num_fake_data_per_class/y_train_num_each_classes)\n",
    "    num_fake_data_array = num_fake_data_array.astype(np.int32)\n",
    "    num_fake_data_total = int(np.sum(num_fake_data_array * y_train_num_each_classes))\n",
    "\n",
    "    print(y_train_num_each_classes)\n",
    "    print(num_fake_data_array)\n",
    "    print(num_fake_data_total)\n",
    "    \n",
    "    X_train_fake = np.zeros([num_fake_data_total, 32, 32, 3], dtype=np.uint8) #Make sure this is uint8. Default datatype messes up the colors\n",
    "    y_train_fake = np.zeros([num_fake_data_total], dtype=np.uint8)\n",
    "    fake_idx = 0\n",
    "    for ii in range(X_train.shape[0]):\n",
    "        for jj in range(num_fake_data_array[y_train[ii]]):\n",
    "            X_train_fake[fake_idx] = transform_image(X_train[ii],10,8,5)\n",
    "            y_train_fake[fake_idx] = y_train[ii]\n",
    "            fake_idx += 1\n",
    "    \n",
    "    print(fake_idx)\n",
    "    is_fake_data_generated = True\n",
    "    \n",
    "print(\"Fake Data Generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show histogram of different image classes\n",
    "y_train_num_each_classes = num_of_each_classes(y_train)\n",
    "print(y_train_num_each_classes)\n",
    "print(np.sum(y_train_num_each_classes))\n",
    "y_train_num_each_classes = num_of_each_classes(y_train_fake)\n",
    "print(y_train_num_each_classes)\n",
    "print(np.sum(y_train_num_each_classes))\n",
    "\n",
    "#Plot a histogram of how many images are of each type\n",
    "plt.hist(y_train, bins=43)\n",
    "plt.title(\"Input Data Histogram\")\n",
    "plt.xlabel(\"Traffic Sign Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "#Plot a histogram of how many images are of each type\n",
    "plt.hist(y_train_fake, bins=43)\n",
    "plt.title(\"Fake Input Data Histogram\")\n",
    "plt.xlabel(\"Traffic Sign Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (is_fake_data_generated):\n",
    "    # Plot n images from the entire fake data set in sequence or in random\n",
    "    n_img = 9\n",
    "    n_img_array = range(0,n_img) #in sequence\n",
    "    #n_img_array = np.random.randint(0, high=n_train, size=n_img) #in random\n",
    "    plot_images(X_train_fake[n_img_array], y_train_fake[n_img_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ((use_fake_data) and not(is_fake_data_concatenated)):\n",
    "    X_train = np.concatenate((X_train, X_train_fake), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_train_fake), axis=0)\n",
    "    is_fake_data_concatenated = True\n",
    "        \n",
    "print(\"Fake Data Concatenated to the Original Data\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a histogram of how many images are of each type in the final training set\n",
    "plt.hist(y_train, bins=43)\n",
    "plt.title(\"Final Input (with Fake Data Added) Histogram\")\n",
    "plt.xlabel(\"Traffic Sign Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n images from the entire training set (incl. fake data if being used) in sequence or in random\n",
    "n_img = 16\n",
    "#n_img_array = range(0,n_img) #in sequence\n",
    "n_img_array = np.random.randint(0, high=n_train, size=n_img) #in random\n",
    "plot_images(X_train[n_img_array], y_train[n_img_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "def standardize_color(image_data, image_mean, image_std):\n",
    "    \"\"\"\n",
    "    Standardize the image data with mean and standard deviation to [-1 to 1]\n",
    "    :param image_data: The image data to be standardized\n",
    "    :return: Standardized image data\n",
    "    \"\"\"\n",
    "    # ToDo: Implement the feature standardization\n",
    "    norm_img = (image_data - image_mean) / image_std\n",
    "    \n",
    "    return norm_img\n",
    "\n",
    "# Normalize the features\n",
    "def normalize_color(image_data):\n",
    "    \"\"\"\n",
    "    Normalize the image data with max and min to -1 to 1\n",
    "    :param image_data: The image data to be normalized\n",
    "    :return: normalized image data\n",
    "    \"\"\"\n",
    "    # ToDo: Implement the feature standardization\n",
    "    norm_img = ((image_data)/255 - 0.5)*2 #so will be between -1 and +1\n",
    "    \n",
    "    return norm_img\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not is_features_normal:\n",
    "    \n",
    "    if standardize_image:\n",
    "        X_mean = np.mean(X_train, axis=0)\n",
    "        X_std = np.std(X_train, axis=0)\n",
    "        X_train = standardize_color(X_train, X_mean, X_std)\n",
    "        X_test = standardize_color(X_test, X_mean, X_std)\n",
    "    \n",
    "        is_features_normal = True\n",
    "    \n",
    "    if normalize_image:\n",
    "        X_train = normalize_color(X_train)\n",
    "        X_test = normalize_color(X_test)\n",
    "    \n",
    "        is_features_normal = True\n",
    "        \n",
    "print('Features have been pre-processed')\n",
    "\n",
    "\n",
    "#print(X_mean.shape) #to verify the mean is taken acorss all the examples\n",
    "#print(X_std.shape) #to verify the standard dev is taken acorss all the examples\n",
    "#for sanity check\n",
    "print(np.max(X_test))\n",
    "print(np.min(X_test))\n",
    "print(np.mean(X_test))\n",
    "print(np.std(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Randomly shuffle the training data. As right now it is ordered and so won't train well.\n",
    "# #Amit's version without using sklearn\n",
    "# if not is_training_data_shuffled:\n",
    "#     n_train_array = np.arange(n_train)\n",
    "\n",
    "#     #shuffle everything thoroughly by shuffling thrice.\n",
    "#     np.random.shuffle(n_train_array); np.random.shuffle(n_train_array); np.random.shuffle(n_train_array)\n",
    "\n",
    "#     X_train = X_train[n_train_array][:][:][:]\n",
    "#     y_train = y_train[n_train_array][:][:][:]\n",
    "    \n",
    "#     is_training_data_shuffled = True\n",
    "\n",
    "# print('Training Data Shuffled')\n",
    "\n",
    "# #Create a validation dataset.\n",
    "# if not is_validation_data:\n",
    "#     n_val = 5000\n",
    "#     X_val = X_train[0:n_val][:][:][:]\n",
    "#     y_val = y_train[0:n_val][:][:][:]\n",
    "\n",
    "#     X_train = X_train[n_val:][:][:][:]\n",
    "#     y_train = y_train[n_val:][:][:][:]\n",
    "\n",
    "#     n_train = X_train.shape[0] #adjustthe training data size.\n",
    "    \n",
    "#     is_validation_data = True\n",
    "    \n",
    "# print('Validation Data Created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomly shuffle the training data. As right now it is ordered and so won't train well.\n",
    "#version using sklearn (it's better than Amit's version above in terms of really shuffling the data)\n",
    "\n",
    "if (not is_training_data_shuffled) and (not is_validation_data) and (not is_test_data_shuffled):\n",
    "\n",
    "    # Get randomized datasets for training and validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        test_size=0.05, #validation set is 5% of the training set\n",
    "        random_state=1) #some seed to help with debugging\n",
    "    \n",
    "    # randomly shuffle the test datasets\n",
    "    X_test, _, y_test, _ = train_test_split(\n",
    "        X_test,\n",
    "        y_test,\n",
    "        test_size=0.0001, #Just to reshufffle the test set. So keep 99.99% of it\n",
    "        random_state=1) #some seed to help with debugging\n",
    "    \n",
    "    is_training_data_shuffled = True\n",
    "    is_test_data_shuffled = True\n",
    "    is_validation_data = True\n",
    "    \n",
    "    \n",
    "print('Training Data Shuffled')\n",
    "print(X_train.shape[0])\n",
    "print('Test Data Shuffled')\n",
    "print(X_test.shape[0])\n",
    "print('Validation Data Created')\n",
    "print(X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We need to 'one-hot' encode the labels.\n",
    "if not is_labels_encod:\n",
    "    # Turn labels into numbers and apply One-Hot Encoding.\n",
    "    encoder = LabelBinarizer()\n",
    "    encoder.fit(np.array(y_train)) #No need to redo this for y_val and y_test as the one-hot encoded vector is same size for all of them\n",
    "    y_train_onehot = encoder.transform(y_train)\n",
    "    y_val_onehot = encoder.transform(y_val)\n",
    "    y_test_onehot = encoder.transform(y_test)\n",
    "\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    y_train_onehot = y_train_onehot.astype(np.float32)\n",
    "    y_val_onehot = y_val_onehot.astype(np.float32)\n",
    "    y_test_onehot = y_test_onehot.astype(np.float32)\n",
    "    \n",
    "    is_labels_encod = True\n",
    "\n",
    "print('Labels One-Hot Encoded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n images from the shuffled and resized training set (note images are normalized)\n",
    "n_img = 9\n",
    "n_img_array = range(0,n_img)\n",
    "plot_images(X_train[n_img_array], y_train[n_img_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for sanity check\n",
    "print(X_train.dtype) #is float64\n",
    "print(y_train.dtype) #is uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_fake_data_generated, 'You skipped the step to generate the fake data'\n",
    "assert use_fake_data, 'Warning: You are not use the fake data for training!'\n",
    "assert is_features_normal, 'You skipped the step to normalize the features'\n",
    "assert is_labels_encod, 'You skipped the step to One-Hot Encode the labels'\n",
    "assert is_training_data_shuffled, 'You skipped the step to randomly shuffle the training data'\n",
    "assert is_validation_data, 'You skipped the step to generate the validation data'\n",
    "assert is_test_data_shuffled, 'You skipped the step to randomly shuffle the test data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1 \n",
    "\n",
    "_Describe the techniques used to preprocess the data._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "In general, feature standardization is more tolerant to outliers compared to feature normalization. In feature normalization, an outlier will really compress the rest of the data together (as we are dividing by the max-min value as opposed to the standard deviation as is the case with feature standardization). Moreover, and really a consequence of the previous advantage, is that standardization more effectively uncoveres similarities/covariance between different features -- as in standardization you maintain the relative distribution of the original data. \n",
    "\n",
    "Having said that, the advantage of normalization is that it limits the data between -1 and +1 whereas standardization limits the standard deviation to that range, and thus some data (and esp. outliers) will fall outside that range. While standardization is generally the preferred approach in many machine learning applications, with images, the pixels values need to fit a given range (e.g. 0 to 255). So normalization is the preferred approach. Moreover, with sigmoid-activation based neural networks, the input needs to be within a given range (e.g. -1 to 1) to prevent gradient saturation.\n",
    "\n",
    "Moreover, when I experimented, with the given data set, normalization increased my validation accuracy by almost 1% compared to using standardization.\n",
    "\n",
    "So here's the approach taken:\n",
    "1. First feature normalization on the training and test set was performed\n",
    "2. Then the data was randomly shuffled and a validation set was generated\n",
    "3. There after the data labels are 'one-hot key' encoded to help with the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Generate data additional (if you want to!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "_Describe how you set up the training, validation and testing data for your model. If you generated additional data, why?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "As shown in the histogram above, the original training data is quite unevenly distributed -- which can potentially affect the test accuracy. To make it more evenly distributed, I generated additional training data. This also significantly increased my training data -- which is usually good.\n",
    "\n",
    "In terms of the validation data, I randomly took 5% of the training data to generate the validation data. This could have been a larger number, but given the training set is so large, even with 5%, I still ended up with about 10,000 examples for the validation set.\n",
    "\n",
    "Lastly, given the initially data were grouped together depending upon the class of the traffic sign, which is bad for stochastic gradient descent (as we take datasets in small batches), I randomly shuffled the training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define your architecture here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input image size in each dimension.\n",
    "img_size = X_train.shape[1] #same for x and y dimensions.\n",
    "num_img_channels = X_train.shape[3] #3 color channels for the input image and 1 for grayscale.\n",
    "img_size_flat = img_size * img_size * num_img_channels #use for placeholder input. Tensorflow can't do placeholder for 4d tensor\n",
    "\n",
    "# Number of classes, for the GTSRB dataset it's 43.\n",
    "num_classes = y_test_onehot.shape[1]\n",
    "\n",
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters1 = 32         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # Convolution filters are 5 x 5 pixels.\n",
    "num_filters2 = 64         # There are 32 of these filters.\n",
    "\n",
    "# Fully-connected layer 1.\n",
    "fc1_size = 1024             # Number of neurons in fully-connected layer1.\n",
    "\n",
    "# Fully-connected layer 2.\n",
    "fc2_size = 256             # Number of neurons in fully-connected layer2\n",
    "\n",
    "\n",
    "# Fully-connected layer 3 (the output layer).\n",
    "fc3_size = num_classes     # Number of neurons in fully-connected layer3/output layer.\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) #This is a global variable. Will be so defined explicitly inside any functions it will be used in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to reshape the input images in to a 2D array for tensorflow placeholder as it can't accept a 4D tensor\n",
    "# Note it is after input normalization\n",
    "X_train_flat = X_train.reshape([-1, img_size_flat]) #-1 means it will automatically calculate it\n",
    "X_val_flat = X_val.reshape([-1, img_size_flat]) #-1 means it will automatically calculate it\n",
    "X_test_flat = X_test.reshape([-1, img_size_flat]) #-1 means it will automatically calculate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "def new_weights(shape,sdev):\n",
    "    weights = tf.truncated_normal(shape=shape, mean=0.0, stddev=sdev, dtype=tf.float32, seed=10)\n",
    "    weights = tf.Variable(weights)\n",
    "    return weights\n",
    "\n",
    "def new_biases(length):\n",
    "    biases = tf.constant(0, dtype=tf.float32, shape=[length])\n",
    "    biases = tf.Variable(biases)\n",
    "    return biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_conv_layer(inpt,\n",
    "                   num_input_channels,\n",
    "                   filter_size,\n",
    "                   num_filters,\n",
    "                   use_pooling=True,\n",
    "                   use_dropout = False):\n",
    "    \n",
    "    #shape of the filter weights.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "    #using Xavier's method to initialize the weights. This is important.\n",
    "    sdev = math.sqrt(2/(filter_size*filter_size*num_input_channels)) #*10 #0.05 #1\n",
    "    #create new weigths.\n",
    "    weights = new_weights(shape=shape, sdev=sdev)\n",
    "    #Create new biases. One for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "    \n",
    "    \n",
    "    # Create the TensorFlow operation for convolution.\n",
    "    # Note the strides are set to 1 in all dimensions.\n",
    "    # The first and last stride must always be 1,\n",
    "    # because the first is for the image-number and\n",
    "    # the last is for the input-channel.\n",
    "    # But e.g. strides=[1, 2, 2, 1] would mean that the filter\n",
    "    # is moved 2 pixels across the x- and y-axis of the image.\n",
    "    # The padding is set to 'SAME' which means the input image\n",
    "    # is padded with zeroes so the size of the output is the same.\n",
    "    layer = tf.nn.conv2d(input=inpt, filter=weights, strides=[1,1,1,1], padding='SAME')\n",
    "        # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer = tf.add(layer, biases)\n",
    "\n",
    "    # Use pooling to down-sample the image resolution by 2X?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling, which means that we\n",
    "        # consider 2x2 windows and select the largest value\n",
    "        # in each window. Then we move 2 pixels to the next window.\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1],\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "    # Rectified Linear Unit (ReLU).\n",
    "    # It calculates max(x, 0) for each input pixel x.\n",
    "    # This adds some non-linearity to the formula and allows us\n",
    "    # to learn more complicated functions.\n",
    "    layer = tf.nn.relu(layer)\n",
    "\n",
    "    # Note that ReLU is normally executed before the pooling,\n",
    "    # but since relu(max_pool(x)) == max_pool(relu(x)) we can\n",
    "    # save 75% of the relu-operations by max-pooling first.\n",
    "\n",
    "    # We return both the resulting layer and the filter-weights\n",
    "    # because we will plot the weights later.\n",
    "    \n",
    "    #Use Dropout?\n",
    "    if use_dropout:\n",
    "        layer = tf.nn.dropout(layer, keep_prob)\n",
    "\n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    # We can use a function from TensorFlow to calculate this.\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    # Note that we just set the size of the second dimension\n",
    "    # to num_features and the size of the first dimension to -1\n",
    "    # which means the size in that dimension is calculated\n",
    "    # so the total size of the tensor is unchanged from the reshaping.\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_fc_layer(inpt,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 use_relu=True,  # Use Rectified Linear Unit (ReLU)?\n",
    "                 use_dropout=False): #Use dropout? (Amit added this code)\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    #Using Xavier's method to initialize weights. This is important.\n",
    "    sdev=math.sqrt(2/num_inputs) #*10 #0.05 #1\n",
    "    weights = new_weights(shape=[num_inputs, num_outputs], sdev=sdev)\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.add(tf.matmul(inpt, weights), biases)\n",
    "\n",
    "    # Use ReLU?\n",
    "    if use_relu:\n",
    "        layer = tf.nn.relu(layer)\n",
    "    \n",
    "    #Use Dropout?\n",
    "    if use_dropout:\n",
    "        layer = tf.nn.dropout(layer, keep_prob)\n",
    "        \n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conv2d expects the input image to be a 4-dimensional tensor.\n",
    "X_img_flat = tf.placeholder(tf.float32, shape=[None, img_size_flat])  #None means it can be of any size\n",
    "X_image = tf.reshape(X_img_flat, shape=[-1, img_size, img_size, num_img_channels]) #-1 means it will automatically calculate\n",
    "#X_image = tf.placeholder(tf.float32, shape=[None, img_size, img_size, num_img_channels]) #doesn't work as placeholder cant be more than 2D array\n",
    "y_true_onehot = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "\n",
    "# fclayer2 = new_fc_layer(input=X_img_flat,\n",
    "#                         num_inputs=img_size_flat,\n",
    "#                         num_outputs=fc2_size,\n",
    "#                         use_relu=True,\n",
    "#                         use_dropout=False)\n",
    "# logits = fclayer2\n",
    "\n",
    "\n",
    "#cvlayer1 will be ? X 16 x 16 x 32\n",
    "cvlayer1, cvweights1 = new_conv_layer(inpt=X_image,\n",
    "                          num_input_channels = num_img_channels,\n",
    "                          filter_size = filter_size1,\n",
    "                          num_filters = num_filters1,\n",
    "                          use_pooling = True,\n",
    "                          use_dropout = True)\n",
    "\n",
    "#cvlayer2 will be ? X 16 x 16 x 64\n",
    "cvlayer2, cvweights2 = new_conv_layer(inpt=cvlayer1,\n",
    "                          num_input_channels = num_filters1,\n",
    "                          filter_size = filter_size2,\n",
    "                          num_filters = num_filters2,\n",
    "                          use_pooling = False,\n",
    "                          use_dropout = True)\n",
    "\n",
    "\n",
    "#But we first need to flatten cvlayer3 as the fully connected layer only accepts a 2 dimensional input.\n",
    "# Then need to combine the outputs of CV1 and CV2\n",
    "cvlayer1_flat, cvlayer1_numfeatures = flatten_layer(cvlayer1)\n",
    "cvlayer2_flat, cvlayer2_numfeatures = flatten_layer(cvlayer2)\n",
    "multipass_output = tf.concat(1, [cvlayer1_flat, cvlayer2_flat])\n",
    "#num_features_multipass = multipass_output.get_shape()[1] #not sure why this doesn't work\n",
    "#num_features_multipass = tf.cast(num_features_multipass, tf.int32) #not sure why this doesn't work\n",
    "num_features_multipass = cvlayer1_numfeatures + cvlayer2_numfeatures #so using this as alternative approach\n",
    "\n",
    "#fclayer1 will be ? X 1024\n",
    "fclayer1, fcweights1 = new_fc_layer(inpt=multipass_output,\n",
    "                        num_inputs=num_features_multipass,\n",
    "                        num_outputs=fc1_size,\n",
    "                        use_relu=True,\n",
    "                        use_dropout=True)\n",
    "\n",
    "#fclayer2 will be ? X 256\n",
    "fclayer2, fcweights2 = new_fc_layer(inpt=fclayer1,\n",
    "                        num_inputs=fc1_size,\n",
    "                        num_outputs=fc2_size,\n",
    "                        use_relu=True,\n",
    "                        use_dropout=True)\n",
    "\n",
    "#fclayer2 will be ? X 43 #Don't use Dropout for output layer. Also don't use ReLU for output layer\n",
    "fclayer3, fcweights3 = new_fc_layer(inpt=fclayer2,\n",
    "                        num_inputs=fc2_size,\n",
    "                        num_outputs=fc3_size,\n",
    "                        use_relu=False,\n",
    "                        use_dropout=False)\n",
    "\n",
    "logits = fclayer3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = tf.constant(0.5e-3, dtype=tf.float32) #it performs better than 3e-3 in terms of accuracy\n",
    "batch_size = 256 #256 #tf.constant(256) #, dtype=tf.float32) #256 #64\n",
    "epochs = 15 #tf.constant(10) #, dtype=tf.float32)\n",
    "lambda_reg = tf.constant(0.0001, dtype=tf.float32) #0.0005 #0.0\n",
    "\n",
    "#keep_prob needs to be 1 when testing accuracy\n",
    "#Limiting the size of the training size for measuring accuracy\n",
    "train_feed_dict = {X_img_flat: X_train_flat[0:500], y_true_onehot: y_train_onehot[0:500], keep_prob: 1}\n",
    "val_feed_dict = {X_img_flat: X_val_flat[0:500], y_true_onehot: y_val_onehot[0:500], keep_prob: 1}\n",
    "test_feed_dict = {X_img_flat: X_test_flat[0:500], y_true_onehot: y_test_onehot[0:500], keep_prob: 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.nn.softmax(logits) #will perform along the last dimension. y_pred is ? X 43\n",
    "y_pred_cls = tf.arg_max(y_pred, dimension=1)\n",
    "y_true_cls = tf.arg_max(y_true_onehot, dimension=1)\n",
    "is_prediction_correct = tf.equal(y_pred_cls, y_true_cls) #boolean vector\n",
    "accuracy = tf.reduce_mean(tf.cast(is_prediction_correct, tf.float32))\n",
    "val_top5, indx_top5 = tf.nn.top_k(y_pred, k=5, sorted=True) #Top 5 predictions\n",
    "\n",
    "#Cross_entropy\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_true_onehot) #will perform along the last dimension\n",
    "\n",
    "loss_unreg = tf.reduce_mean(cross_entropy) #loss is a scalar\n",
    "loss_reg = lambda_reg*(tf.nn.l2_loss(cvweights1) + tf.nn.l2_loss(cvweights2) + tf.nn.l2_loss(fcweights1) + tf.nn.l2_loss(fcweights2) + tf.nn.l2_loss(fcweights3))\n",
    "loss = tf.add(loss_unreg, loss_reg)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "_What does your final architecture look like? (Type of model, layers, sizes, connectivity, etc.)  For reference on how to build a deep neural network using TensorFlow, see [Deep Neural Network in TensorFlow\n",
    "](https://classroom.udacity.com/nanodegrees/nd013/parts/fbf77062-5703-404e-b60c-95b78b2f3f9e/modules/6df7ae49-c61c-4bb2-a23e-6527e69209ec/lessons/b516a270-8600-4f93-a0a3-20dfeabe5da6/concepts/83a3a2a2-a9bd-4b7b-95b0-eb924ab14432) from the classroom._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "My final arichtecture is as follows:\n",
    "1. Conv layer 1 with pooling, dropout, and ReLU activation with 32 filters\n",
    "2. Conv layer 2 with dropout and ReLU activation with 64 filters (no pooling). Note that the output of both conv layers are feed to the first fully connected layer for better accuracy (ref. Lecun paper mentioned above)\n",
    "3. Fully connected layer 1 with 1024 neurons with dropout and ReLU activation. The input to FC1 is fed from both, conv layers 1 and 2.\n",
    "4. Fully connected layer 2 with 256 neurons with dropout and ReLU activation\n",
    "5. Fully connected layer 3 (or the output layer) with 43 neurons with no dropout nor ReLU activation. Didn't use dropout on the output layer cause we always need to make predicitons about all the 43 different classes. And typically not using non-linearity (activation) in the output layer gives better performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#In order to save the variables of the neural network, we now create a so-called Saver-object which is used for storing \n",
    "#and retrieving all the variables of the TensorFlow graph. Nothing is actually saved at this point\n",
    "saver = tf.train.Saver()\n",
    "save_dir = 'checkpoints/' #This is the directory used for saving and retrieving the data.\n",
    "\n",
    "#Create the directory if it doesn't exist\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "    \n",
    "#This is the path for the files to be saved/restored\n",
    "save_path = save_dir + 'best_validation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the initial loss and accuracy for sanity check\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    acc_init, loss_init = session.run([accuracy, loss], feed_dict=val_feed_dict)\n",
    "    print(\"Initial accuracy: {0:.1%}\".format(acc_init)) #will be ~(1/43) = 2.3%\n",
    "    print(\"Initial loss: {0:.5}\".format(loss_init)) #will be ~-ln(1/43) = 3.7 (for unregularized softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    \n",
    "    global keep_prob #This is a global variable\n",
    "    #No need to set save_path as global variable. Just define it before using it everytime as local variable.\n",
    "    \n",
    "    session.run(init) #no need to initialize always\n",
    "    \n",
    "    #Re-load all the variables that were saved to file during previous optimization.\n",
    "    #This will initialize the weights to more optimum values -- faster training\n",
    "    save_path = save_dir + 'best_validation' #save_path is local variable here\n",
    "    #saver.restore(sess=session, save_path=save_path)\n",
    "        \n",
    "    batch_count = int(math.ceil(X_train_flat.shape[0]/batch_size))\n",
    "    # Start-time used for printing time-usage below.\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        \n",
    "        for batch_i in range(batch_count):\n",
    "            \n",
    "            batch_start = batch_i * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch_images = X_train_flat[batch_start:batch_end]\n",
    "            batch_labels = y_train_onehot[batch_start:batch_end]\n",
    "            \n",
    "            #Run the optimizer and get the loss\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X_img_flat: batch_images, y_true_onehot: batch_labels, keep_prob: 0.5})\n",
    "            \n",
    "        \n",
    "        #print status every epoch\n",
    "        acc_train = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "        acc_val = session.run(accuracy, feed_dict=val_feed_dict)\n",
    "        acc = session.run(accuracy, feed_dict=test_feed_dict)\n",
    "        # Message for printing.\n",
    "        msg = \"Epoch {0} => \\t Training Accuracy: {1:>6.1%}\"\n",
    "        print(msg.format(epoch_i + 1, acc_train))\n",
    "        msg = \"Epoch {0} => \\t Validation Accuracy: {1:>6.1%}\"\n",
    "        print(msg.format(epoch_i + 1, acc_val))\n",
    "        msg = \"Epoch {0} => \\t Test Accuracy: {1:>6.1%}\\n\"\n",
    "        print(msg.format(epoch_i + 1, acc))\n",
    "\n",
    "    end_time = time.time()\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "    \n",
    "    # Save all variables of the TensorFlow graph to file.\n",
    "    save_path = save_dir + 'best_validation' #save_path is local variable here\n",
    "    saver.save(sess=session, save_path=save_path)\n",
    "    \n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final run's training loss: {0:.5}\".format(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    save_path = save_dir + 'best_validation' #save_path is local variable here\n",
    "    saver.restore(sess=session, save_path=save_path)    \n",
    "    train_feed_dict = {X_img_flat: X_train_flat[0:1000], y_true_onehot: y_train_onehot[0:1000], keep_prob: 1}\n",
    "    acc = session.run(accuracy, feed_dict=train_feed_dict)\n",
    "    print(\"Final Training accuracy: {0:.2%}\".format(acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cls_pred, cls_true):\n",
    "\n",
    "    # cls_pred is an array of the predicted class-number for\n",
    "    # all images in the test-set.\n",
    "    # cls_true is an array of the actual (ground truth) class-number for\n",
    "    # all images in the test-set.\n",
    "    \n",
    "    \n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=cls_true,\n",
    "                          y_pred=cls_pred)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.matshow(cm)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.colorbar()\n",
    "#     tick_marks = np.arange(n_classes)\n",
    "#     plt.xticks(tick_marks, range(n_classes))\n",
    "#     plt.yticks(tick_marks, range(n_classes))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    save_path = save_dir + 'best_validation' #save_path is local variable here\n",
    "    saver.restore(sess=session, save_path=save_path)  \n",
    "    \n",
    "    acc = np.zeros(6)\n",
    "    for i in range(6):\n",
    "        start = 2000 * i\n",
    "        stop = start + 2000\n",
    "        test_feed_dict = {X_img_flat: X_test_flat[start:stop], y_true_onehot: y_test_onehot[start:stop], keep_prob: 1}\n",
    "        acc[i], ytest_pred_cls, ytest_true_cls = session.run([accuracy, y_pred_cls, y_true_cls], feed_dict=test_feed_dict)\n",
    "        print(\"Test accuracy {0}: {1:>6.1%}\".format(i+1, acc[i]))\n",
    "    \n",
    "    print(\"Mean Test accuracy: {0:.1%}\".format(np.mean(acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot of a confusion matrix of the results from the last batch of test data above\n",
    "#For each true class, it shows which are the different predicitons the model is making\n",
    "\n",
    "plot_confusion_matrix(ytest_pred_cls, ytest_true_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unstandardize_color(norm_image, image_mean, image_std):\n",
    "    \"\"\"\n",
    "    Un-standardize the image data with mean and standard deviation\n",
    "    :param norm_image: The image data to be un-standardized\n",
    "    :return: Un-standardized image data\n",
    "    \"\"\"\n",
    "    # ToDo: Implement feature un-normalization (to restore the image to its original form)\n",
    "    un_norm_image = (norm_image * image_std) + image_mean #it is float32\n",
    "    un_norm_image = un_norm_image.astype(np.uint8) #need to cast to uint8 otherwise the image looks messed-up\n",
    "    \n",
    "    return un_norm_image\n",
    "\n",
    "def unnormalize_color(norm_image):\n",
    "    \"\"\"\n",
    "    Un-normalize the image data\n",
    "    :param norm_image: The image data to be un-normalized\n",
    "    :return: Un-normalize image data\n",
    "    \"\"\"\n",
    "    un_norm_image = (norm_image/2+0.5)*255 #it is float32\n",
    "    un_norm_image = un_norm_image.astype(np.uint8) #need to cast to uint8 otherwise the image looks messed-up\n",
    "    return un_norm_image\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n images from the shuffled test test to check the accuracy of the predictions\n",
    "n_img = 9\n",
    "n_img_array = range(0,n_img) #in sequence\n",
    "#n_img_array = np.random.randint(0, high=n_test, size=n_img) #in random\n",
    "\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    save_path = save_dir + 'best_validation' #save_path is local variable here\n",
    "    saver.restore(sess=session, save_path=save_path)  \n",
    "    \n",
    "    test_feed_dict = {X_img_flat: X_test_flat[n_img_array], y_true_onehot: y_test_onehot[n_img_array], keep_prob: 1}\n",
    "    y_pred_test, y_true_test = session.run([y_pred_cls, y_true_cls], feed_dict=test_feed_dict)\n",
    "\n",
    "if standardize_image:\n",
    "    X_test_unnorm = unstandardize_color(X_test, X_mean, X_std)\n",
    "\n",
    "if normalize_image:\n",
    "    X_test_unnorm = unnormalize_color(X_test)\n",
    "    \n",
    "plot_images(X_test_unnorm[n_img_array], y_true_test, y_pred_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "_How did you train your model? (Type of optimizer, batch size, epochs, hyperparameters, etc.)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "I trained my model using the Adam optimizer as it adaptively adjusts the learning rate to allow for faster convergence and good accuracy. Moreover, the Adam optimizer uses momentum update to update the weights and biases so as to mitigate any noise in the gradients of a given batch from significantly affecting (or throwing off) the training direction. In a nutshell, Adam optimizer is RMSProp with momentum update, so the best optimizer we currently have.\n",
    "\n",
    "I am using a batch size of 256, 15 epochs, dopout of 0.5 (during training) and lambda (L2 regularization) of 0.0001. Given these are all hyperparameters, to arrive at these values, I initially started off with some rule of thumb values for these parameters (or from the lecture notes) and then I adjusted them in a coarse manner (larger steps) to see if it is making significant difference, and when it did, and I then changed those parameters in finer steps.\n",
    "\n",
    "For the regularization parameter, in addition to observing improvements in the validation accuracy, I was also looking at the difference between training and validation/test accuracies, as a larger difference betwen them suggests strong overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "\n",
    "_What approach did you take in coming up with a solution to this problem?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "I started off with the ConvNet architecture used in Magnus Pedersen's tensorflow tutorials and then improved upon it based on the Udacity lectures and well as the LeCun paper referenced above. The Lecun paepr was an intersting read as it helped me learn about combining the output of different conv layers before feeding to the first fully connected layer for better accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Test a Model on New Images\n",
    "\n",
    "Take several pictures of traffic signs that you find on the web or around you (at least five), and run them through your classifier on your computer to produce example results. The classifier might not recognize some local signs but it could prove interesting nonetheless.\n",
    "\n",
    "You may find `signnames.csv` useful as it contains mappings from the class id (integer) to the actual sign name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Use the code cell (or multiple code cells, if necessary) to implement the first step of your project. Once you have completed your implementation and are satisfied with the results, be sure to thoroughly answer the questions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the images and plot them here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file of sign_names\n",
    "sign_names_files = 'signnames.csv'\n",
    "class_ID = []\n",
    "sign_name = []\n",
    "with open(sign_names_files) as csv_file:\n",
    "    reader = csv.DictReader(csv_file)\n",
    "    for row in reader:\n",
    "        class_ID.append(row['ClassId'])\n",
    "        sign_name.append(row['SignName'])\n",
    "        print(row['ClassId'], row['SignName'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is only needed if I am trying to automatically determine the accuracy of the model's prediction on the test images\n",
    "def one_hot_encode(true_label_class):\n",
    "    one_hot_vec = np.zeros(43)\n",
    "    one_hot_vec[true_label_class] = 1\n",
    "    # Change to float32, so it can be multiplied against the features in TensorFlow, which are float32\n",
    "    one_hot_vec = one_hot_vec.astype(np.float32)\n",
    "    return one_hot_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outline\n",
    "# 1. load the images\n",
    "# 2. plot the images\n",
    "# 3. re-size the images\n",
    "# 4. pre-process the images (input normalization)\n",
    "# 5. Feed image to the neural network for inference\n",
    "\n",
    "def classify_image(img):    \n",
    "    #plt.imshow(img)\n",
    "    new_dim = (32,32)\n",
    "    img_resized = cv2.resize(img, new_dim, interpolation=cv2.INTER_AREA)\n",
    "    # X_mean and X_std are the mean and standard deviations of the entire training set\n",
    "    plt.imshow(img_resized)\n",
    "    plt.show()\n",
    "    \n",
    "    if standardize_image:\n",
    "        img_normalized = standardize_color(img_resized, X_mean, X_std)\n",
    "    if normalize_image:\n",
    "        img_normalized = normalize_color(img_resized)\n",
    "        \n",
    "    img_norm_flat = img_normalized.reshape(-1, img_size_flat)\n",
    "    \n",
    "    with tf.Session() as session:\n",
    "        session.run(init)\n",
    "        save_path = save_dir + 'best_validation' #save_path is local variable here\n",
    "        saver.restore(sess=session, save_path=save_path)  \n",
    "\n",
    "        #we don't need to provide y_train_oneshot if we don't want to know the accuracy\n",
    "        prediction = session.run(y_pred_cls, feed_dict={X_img_flat: img_norm_flat, keep_prob: 1})\n",
    "        img_5, img_id_5 = session.run([val_top5, indx_top5], feed_dict={X_img_flat: img_norm_flat, keep_prob: 1})\n",
    "\n",
    "        #print(img_id_5)\n",
    "        #print(img_5)\n",
    "        #print(prediction) #The prediction should be 14 for stop sign. Instead it predicts 12\n",
    "        pred_desc = sign_name[prediction]\n",
    "        return prediction, pred_desc, img_5, img_id_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#To test specific class of images from the test data\n",
    "img_cls_test = 12\n",
    "test_image_id = (y_test == img_cls_test)\n",
    "test_image_id = np.where(test_image_id)\n",
    "test_image_id = test_image_id[0][200]\n",
    "test_image = X_test_unnorm[test_image_id] #using the un-normalized test set\n",
    "\n",
    "prediction, description, top5_prob, top5_ID = classify_image(test_image)\n",
    "print('Test image named {0} is predicted to be: {1} ({2})'.format(img_cls_test, prediction[0], description))\n",
    "print('The top 5 predictions are:')\n",
    "for id5, prob5 in zip(top5_ID[0], top5_prob[0]):\n",
    "    #print(id5)\n",
    "    print('{0} ({2}) with probability of {1:.1%}'.format(id5,prob5,sign_name[id5]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "_Choose five candidate images of traffic signs and provide them in the report. Are there any particular qualities of the image(s) that might make classification difficult? It would be helpful to plot the images in the notebook._\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "Please see below for the results of five test images taken off the web. In terms of the qualities in the image that can make classification difficult, one thing is that many traffic signs in the US are different from the German traffic signs which the model is trained on, so this will be one difficulty. Another difficulty I have noticed is because the model is trained using a 32x32 pixels colored image, and since most off the web images I could find has over 300 x 300 pixels, this meant I had to resize them image to 32x32 pixels first -- thereby making the image blurry. And this caused difficulty in classify the images even though the test accuracy is 92%. I somewhat addressed this by generating additional training data by randomly jittering (rotating, translating, or skewing the image). Given more time, another thing to do is generate fake data with randomly varying brightness and contrast to further improve real images' accuracy.\n",
    "\n",
    "In summary, because of the above mentioned reasons, even though the validation accuracy is 96.5% and test accuracy is 92.5%, the accuracy on the below five test images is merely 80%.\n",
    "\n",
    "Please note that in order to resize the image, I decided against cropping the image because it won't generalize very well with any image as the traffic sign could be anywhere in the image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Run the predictions here.\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_images = os.listdir('test_traffic_signs/')\n",
    "\n",
    "for idx,val in enumerate(test_images):\n",
    "    #reading images\n",
    "    test_image = mpimg.imread('test_traffic_signs/' + test_images[idx])    \n",
    "    prediction, description, top5_prob, top5_ID = classify_image(test_image)\n",
    "    print('Test image named {0} is predicted to be: {1} ({2})'.format(test_images[idx], prediction[0], description))\n",
    "    print('The top 5 predictions are:')\n",
    "    for id5, prob5 in zip(top5_ID[0], top5_prob[0]):\n",
    "        #print(id5)\n",
    "        print('{0} ({2}) with probability of {1:.1%}'.format(id5,prob5,sign_name[id5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "_Is your model able to perform equally well on captured pictures or a live camera stream when compared to testing on the dataset?_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "No the model performs much better on the test data than the images I downloaded from the web because of the two reasons I mentioned in question 7 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Visualize the softmax probabilities here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "\n",
    "# Please see above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "*Use the model's softmax probabilities to visualize the **certainty** of its predictions, [`tf.nn.top_k`](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#top_k) could prove helpful here. Which predictions is the model certain of? Uncertain? If the model was incorrect in its initial prediction, does the correct prediction appear in the top k? (k should be 5 at most)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The confusion matrix above shows a good overview of which images the model is having hard time predicting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "_If necessary, provide documentation for how an interface was built for your model to load and classify newly-acquired images._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "The basic interface to test on newly acquired images is as follows:\n",
    "1. Save all the test images (jpeg format only) in a directory named 'test_traffic_signs'\n",
    "2. The above code will run through each image in this dircetory\n",
    "3. Every image will be resized to 32x32 pixels\n",
    "4. Then each image will be normalized using the mean and standard deviation values from the training set (upon which the model was trained)\n",
    "5. Thereafter the normalized image is feed to the ConvNet for prediction. And it displays the class of the prediction as well as it's decription.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
